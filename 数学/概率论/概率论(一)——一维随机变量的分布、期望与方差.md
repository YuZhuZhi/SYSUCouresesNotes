# 概率论(一)——一维随机变量的分布、期望与方差

- [概率论(一)——一维随机变量的分布、期望与方差](#概率论一一维随机变量的分布期望与方差)
  - [一、基本知识](#一基本知识)
    - [1.事件间的关系与运算](#1事件间的关系与运算)
    - [2.条件概率与乘法原理](#2条件概率与乘法原理)
    - [3.全概率公式与贝叶斯公式](#3全概率公式与贝叶斯公式)
    - [4.独立性](#4独立性)
  - [二、离散型随机变量与分布](#二离散型随机变量与分布)
    - [1.伯努利分布](#1伯努利分布)
    - [2.二项分布](#2二项分布)
    - [3.泊松分布](#3泊松分布)
  - [三、连续型随机变量与分布](#三连续型随机变量与分布)
    - [1.分布函数与概率密度函数](#1分布函数与概率密度函数)
    - [2.均匀分布](#2均匀分布)
    - [3.指数分布](#3指数分布)
    - [4.正态分布](#4正态分布)
  - [四、随机变量的期望与方差](#四随机变量的期望与方差)
    - [1.期望](#1期望)
    - [2.方差](#2方差)
    - [3.切比雪夫不等式](#3切比雪夫不等式)
    - [4.几种分布的期望与方差](#4几种分布的期望与方差)
  - [五、随机变量的函数的分布与期望](#五随机变量的函数的分布与期望)
    - [1.分布](#1分布)
    - [2.期望](#2期望)

这篇文章中只局限在**单一随机变量**的情况下。首先介绍基本知识。

## 一、基本知识

### 1.事件间的关系与运算

假设有两个事件分别记为$A,B$：

(1)**和事件**：事件$A,B$的和事件，记为$A\cup B$。当两个事件中**至少有一个发生**时，和事件发生；

>多个事件的和事件，我们可以使用集合的**广义并**来表示，记为：
>$$A_1\cup A_2\cup...\cup A_n=\displaystyle\bigcup_{i=1}^nA_i$$

(2)**积事件**：事件$A,B$的积事件，记为$A\cap B$，又简记为$AB$。当两个事件**同时发生**时，积事件发生；

>多个事件的积事件，我们可以使用集合的**广义交**来表示，记为：
>$$A_1\cap A_2\cap...\cap A_n=\displaystyle\bigcap_{i=1}^nA_i$$

(3)**差事件**：事件$A,B$的差事件，记为$A-B$。当$A$事件发生且$B$事件不发生时，差事件发生；

(4)**互斥**：若$A\cap B=\varnothing$，即两者的积事件不发生，即两者不能同时发生，称两事件是互斥的，或者称为**互不相容**的；

(5)**对立**：若$A\cap B=\varnothing$且$A\cup B=S$——这里$S$的意思是样本空间——即两事件互斥、并且构成样本空间全体，即要么发生$A$事件、要么发生$B$事件、二者必居其一，那么称两者**互为对立事件**，或者称**互为逆事件**。可以记为$A=\bar B,B=\bar A$。

由集合的运算，可以推知下列概率基本公式：

(1)若有两个**任意**事件$A,B$，那么他们的和事件如下计算，称为**加法公式**：

$$P(A\cup B)=P(A)+P(B)-P(AB)$$

>有些时候我们不止处理两个事件，因此给出多个事件情境下的加法公式：
$$\begin{aligned}
    \displaystyle P\left(\bigcup_{i=1}^nA_i\right)&=P(A_1\cup A_2\cup...\cup A_n)\\
    &=\sum_{i=1}^{n}P(A_i)-\sum_{1\leq i<j\leq n}P(A_iA_j)+\\
    &\qquad\sum_{1\leq i<j<k\leq n}P(A_iA_jA_k)-...+(-1)^{n-1}P(A_1A_2...A_n)
\end{aligned}$$

(2)若两事件$A,B$为**互斥事件**，那么$P(AB)=P(\varnothing)=0$；

(3)由(1)，(2)，若多个事件$A_1,A_2,...,A_n$均**互斥**，那么这些事件的和事件概率可以简单计算，称为**有限可加性**：

$$\displaystyle P\left(\bigcup_{i=1}^nA_i\right)=P(A_1\cup A_2\cup...\cup A_n)=P(A_1)+P(A_2)+...+P(A_n)$$

(4)若两事件$A,B$**互相对立**，那么由(3)，$P(A\cup B)=P(S)=P(A)+P(B)=1$。也就是，$P(B)=1-P(A)$，或写为$P(\bar A)=1-P(A)$；

(5)若两事件$A,B$间有关系$A\subset B$，那么两者的**差事件**可简单计算：

$$P(B-A)=P(B)-P(A)$$

### 2.条件概率与乘法原理

上述的概率基本公式大部分都是在对和事件进行讨论，为了讨论积事件，引入条件概率。

若有两事件$A,B$，那么在$A$发生的条件下发生$B$的概率，称为**条件概率**，记为$P(B|A)$，并如下计算：

$$P(B|A)=\dfrac{P(AB)}{P(A)}$$

显然这要求$A$是可发生事件。简单对上式做移项，就可以写为$P(AB)=P(A)P(B|A)$，此式称为**乘法公式**。乘法公式易于推广到多事件，并且推广之后形式很好记：

$$P(A_1A_2...A_n)=P(A_n|A_1A_2...A_{n-1})\cdot P(A_{n-1}|A_1A_2...A_{n-2})\cdot...\cdot P(A_2|A_1)\cdot P(A_1)$$

### 3.全概率公式与贝叶斯公式

有时候实验中要发生的事件概率不好直接计算，但是在给定的条件下却能容易求出，这就要求通过条件概率来计算事件概率。这就是全概率公式运用的地方。

但是，我们显然需要让条件概率中的条件事件**覆盖**整个样本空间。一般而言为了简便起见，我们要求对于覆盖样本空间的这一组事件，是样本空间的**划分**。通俗地说，即要求这些事件间**互斥**、但这一组事件的**和事件**恰好是样本空间。

>以数学的语言来说，即一组事件$A_1,A_2,...,A_n$，若(1)$A_iA_j=\varnothing,i\neq j$；(2)$\displaystyle\bigcup_{i=1}^nA_i=A_1\cup A_2\cup...\cup A_n=S$，那么这组事件称为样本空间的一个**划分**。

那么若事件$A\subset S$，并且样本空间$S$有一个划分$B_1,B_2,...,B_n$，则事件$A$的概率就能由**全概率公式计算**：

$$\begin{aligned}
    P(A)&=P(AB_1)+P(AB_2)+...+P(AB_n)\\
    &=P(A|B_1)P(B_1)+P(A|B_2)P(B_2)+...+P(A|B_n)P(B_n)
\end{aligned}$$

由此，当$A$作为条件事件来求划分中某一事件的发生概率时，就能这样计算：

$$\begin{aligned}
    P(B_j|A)&=\dfrac{P(AB_j)}{P(A)}=\dfrac{P(AB_j)}{\sum_{i=1}^nP(AB_i)}\\
    &=\dfrac{P(A|B_j)P(B_j)}{\displaystyle\sum_{i=1}^{n}P(A|B_i)P(B_i)}
\end{aligned}$$

上式称为**贝叶斯公式**。

### 4.独立性

有时事件$A,B$的积事件会出现$P(AB)=P(A)P(B)$的情况，即$P(A|B)=P(A)$或$P(B|A)=P(B)$，即某一事件的发生**并不影响**另一事件的发生，这时我们称两事件是**相互独立**的。

---

## 二、离散型随机变量与分布

开玩笑地说，“随机变量”这个东西有一点量子力学的意味，即“在被观测之后会落入确定的状态”，使得它虽有“变量”之名却有“常量”的表现，使我们常常用**大写字母**$X,Y,Z...$来表示随机变量。但是随机变量的取值又是一个未知的变量，这个变量用**小写字母**表示。因此，表示一个随机变量取值为某值的概率，记为$P(X=x)$。

所谓**离散型随机变量**，是指可能取值只有**有限个**、或是**可列无限个**的随机变量。以下总结一些常用的离散型随机变量及其分布律。

### 1.伯努利分布

**伯努利分布**这个名字来源于伯努利试验，这种试验指的是结果只有两种的试验；伯努利分布又称为$0-1$**分布**，这个别名来源于此处的随机变量取值只能为$0$或$1$。显然，这可以写为两个式子：

$$\begin{cases}
    P(X=0)=1-p\\
    P(X=1)=p\\
\end{cases}$$

由于幂的性质，上两式可以巧妙地合并为同一个公式$P(X=k)=p^k(1-p)^{1-k}$。

### 2.二项分布

假设伯努利试验的结果为$A,\bar A$，发生的概率分别为$p,1-p$。如果将试验**独立重复**地进行$n$次，那么其中一部分结果为$A$，另一部分为$\bar A$。假设随机变量$X$描述的是结果为$A$的次数，那么显然：

$$P(X=k)=C_n^kp^k(1-p)^{n-k}$$

可以看出，这种分布可以由参数$n,p$唯一地确定，称之为**二项分布**，简记为$X\sim b(n,p)$。如果取$n=1$，二项分布就退化为伯努利分布，也就是说只做一次试验时二项分布就是伯努利分布。

### 3.泊松分布

在二项分布中，当试验次数极多、即$n$非常大的时候，使用二项分布的分布律来计算概率已经非常困难。为了简化计算，将二项分布“进化”为**泊松分布**。

注意到$lim_{n\rightarrow+\infty}\left( 1+\dfrac{1}{n} \right)^n=e$，或者写为$lim_{n\rightarrow+\infty}\left( 1-\dfrac{1}{n} \right)^{-n}=e$。这似乎允许我们改写二项分布律：

$$P(X=k)=C_n^kp^k(1-p)^{n-k}$$

中的$(1-p)^{n-k}$项！为此，假设$p=\dfrac{\lambda}{n}$，于是：

$$\begin{aligned}
    P(X=k)&=C_n^kp^k(1-p)^{n-k}\\
    &=C_n^k\cdot \left( \dfrac{\lambda}{n} \right)^k\cdot\left( 1-\dfrac{\lambda}{n} \right)^{n-k}\\
    &=C_n^k\cdot \left( \dfrac{\lambda}{n} \right)^k\cdot\left( 1-\dfrac{\lambda}{n} \right)^{n}\cdot\left( 1-\dfrac{\lambda}{n} \right)^{-k}\\
    &=\left(\dfrac{n(n-1)(n-2)...(n-k+1)}{k!}\cdot \dfrac{\lambda^k}{n^k} \right)\cdot\left( 1-\dfrac{\lambda}{n} \right)^{-k}\cdot\left( 1-\dfrac{\lambda}{n} \right)^{-n}\\
    &=\dfrac{\lambda^k}{k!}\cdot\left( \dfrac{n(n-1)(n-2)...(n-k+1)}{n^k} \right)\cdot\left( 1-\dfrac{\lambda}{n} \right)^{-k}\cdot\left( 1-\dfrac{\lambda}{n} \right)^{-n}\\
    &=\dfrac{\lambda^k}{k!}\cdot\left( 1\cdot\dfrac{n-1}{n}\cdot\dfrac{n-2}{n}\cdot...\cdot\dfrac{n-k+1}{n} \right)\cdot\left( 1-\dfrac{\lambda}{n} \right)^{-k}\cdot\left( 1-\dfrac{\lambda}{n} \right)^{-n}
\end{aligned}$$

当$n$非常大时，数学上的**理想情况**即令$n\rightarrow+\infty$。这也将导致概率$p$非常小。那么上式中：

$$\begin{cases}
    \displaystyle\lim_{n\rightarrow+\infty}\dfrac{n-?}{n}=1\\
    \displaystyle\lim_{n\rightarrow+\infty}\left( 1\cdot\dfrac{n-1}{n}\cdot\dfrac{n-2}{n}\cdot...\cdot\dfrac{n-k+1}{n} \right)=1\\
    \displaystyle\lim_{n\rightarrow+\infty}\left( 1-\dfrac{\lambda}{n} \right)^{-k}=1\\
    \displaystyle\lim_{n\rightarrow+\infty}\left( 1-\dfrac{\lambda}{n} \right)^{-n}=e^{-\lambda}
\end{cases}$$

从而对于二项分布律存在：

$$\displaystyle\lim_{n\rightarrow+\infty}C_n^kp^k(1-p)^{n-k}=_{p=\frac{\lambda}{n}}\dfrac{\lambda^ke^{-\lambda}}{k!}$$

上式称为**泊松定理**，由此导出的分布律称为**泊松分布**。泊松分布显然由参数$\lambda$唯一确定，将这个分布简记为$X\sim \pi(\lambda)$。

当然，由于上述是在理想情况下的推导，要想将二项分布进化为泊松分布，一般要求$n$较大且$p$甚小。推荐在$n\geq20,p\leq0.05$时用参数为$\lambda=np$的泊松分布来近似二项分布。

---

## 三、连续型随机变量与分布

对于**连续型随机变量**，由于其取值可以是数轴上的任意一个值，因此其取值为某值的概率都是$0$。因此，我们对于连续型随机变量关心的是其取值落在某**区间**上的概率。为了方便研究，需要引入**分布函数**与**概率密度函数**。

### 1.分布函数与概率密度函数

**分布函数**刻画的是随机变量**小于**某值的概率，即定义分布函数$F(x)$为：

$$F(x)=P(X\leq x)\quad,\quad-\infty<x<+\infty$$

由上述定义，显然随机变量落在某区间$(x_1,x_2]$上的概率为：

$$P(x_1<X\leq x_2)=P(X\leq x_2)-P(X\leq x_1)=F(x_2)-F(x_1)$$

>事实上，由于随机变量取确定值的概率为$0$，上述的落在某“区间”，无需关注区间的开闭性。

由此，分布函数必然是(非严格)**递增**的**非负**函数。并且：

$$\displaystyle\lim_{x\rightarrow-\infty}F(x)=0\quad,\quad \lim_{x\rightarrow+\infty}F(x)=1$$

分布函数也可以应用于离散型随机变量上，只是连续型的分布函数是连续的、离散型的分布函数是间断的。但**概率密度函数**不行，因为定义概率密度函数、简称概率密度$f(x)$为：

$$F(x)=\displaystyle\int_{-\infty}^{x}f(t)\ dt\quad, \quad f(x)=F'(x)$$

也即概率密度函数是分布函数的导函数。和勒让德变换不同，单纯的求导必然丢失信息，离散型随机变量的概率密度函数必然处处为$0$。

>这里我们忽略了间断点处的导数定义问题，也就是，我们应当规定只有在连续的情况下才允许$f(x)=F'(x)$，从而避免在离散型随机变量中引入概率密度。

这样，若连续型随机变量的取值落在某区间$(x_1,x_2]$上，那么也可以由概率密度来计算其概率：

$$\begin{aligned}
    P(x_1<X\leq x_2)&=F(x_2)-F(x_1)\\
    &=\displaystyle\int_{-\infty}^{x_2}f(t)\ dt-\int_{-\infty}^{x_1}f(t)\ dt\\
    &=\int_{x_1}^{x_2}f(t)\ dt
\end{aligned}$$

因而，由$\lim_{x\rightarrow+\infty}F(x)=1$可得：

$$\int_{-\infty}^{+\infty}f(t)=\lim_{x\rightarrow+\infty}F(x)=1$$

上式表明了概率的**归一性**。

对于连续型随机变量，我们多用概率密度来刻画其分布律。分布函数多用于建立概率与概率密度之间的桥梁。

### 2.均匀分布

所谓**均匀分布**，即指随机变量$X$**等可能**地取区间上的任一值。假设区间为$(a,b)$，那么显然分布函数写为：

$$F(x)=\begin{cases}
    0&x<a\\
    \dfrac{x-a}{b-a}\qquad&a\leq x<b\\
    1&x\geq b\\
\end{cases}$$

求导后得概率密度：

$$f(x)=\begin{cases}
    \dfrac{1}{b-a}\qquad&a<x<b\\
    0&Other\ Cases\\
\end{cases}$$

这种分布由参数$a,b$唯一确定，简记为$X\sim U(a,b)$。

### 3.指数分布

**指数分布**的概率密度具有如下形式：

$$f(x)=\begin{cases}
    \dfrac{1}{\theta}e^{-\frac{x}{\theta}}\qquad&x>0\\
    0&Other\ Cases\\
\end{cases}$$

积分后得分布函数：

$$F(x)=\begin{cases}
    1-e^{-\frac{x}{\theta}}\qquad&x>0\\
    0&Other\ Cases\\
\end{cases}$$

指数分布具有**无记忆性**，这是说：

$$\begin{aligned}
    P(X>s+t|X>s)&=\dfrac{P(X>s+t\ \cap\ X>s)}{P(X>s)}\\
    &=\dfrac{P(X>s+t)}{P(X>s)}\\
    &=\dfrac{e^{-\frac{s+t}{\theta}}}{e^{-\frac{s}{\theta}}}\\
    &=e^{-\frac{t}{\theta}}\\
    &=P(X>t)
\end{aligned}$$

### 4.正态分布

**正态分布**的概率密度函数写为：

$$f(x)=\dfrac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\quad,\quad-\infty<x<+\infty$$

正态分布由参数$\mu,\sigma^2$唯一确定，简记为$X\sim N(\mu, \sigma^2)$。这两个参数的意义分别是随机变量的**期望**与**方差**。分布函数是概率密度的积分，但这个函数没有初等函数解，只能直接表示为：

$$F(x)=\displaystyle\dfrac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{x}e^{-\frac{(t-\mu)^2}{2\sigma^2}}\ dt\quad,\quad-\infty<x<+\infty$$

若随机变量$Z\sim N(\mu, \sigma^2)$，那么**线性变换**之后可以使其概率密度与分布函数变换为更规整的形式。令$X=\dfrac{Z-\mu}{\sigma}$，那么：

$$f(x)=\dfrac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\quad,\quad-\infty<z<+\infty$$

$$F(x)=\displaystyle\dfrac{1}{\sqrt{2\pi}}\int_{-\infty}^{x}e^{-\frac{t^2}{2}}\ dt\quad,\quad-\infty<z<+\infty$$

即相当于令$\mu=0,\sigma = 1$，这种正态分布称为**标准正态分布**，分别用$\varphi(x),\varPhi(x)$表示它的概率密度和分布函数。任意的正态分布都能变换为标准正态分布，这为计算概率提供了极大方便。对于随机变量$X\sim N(\mu, \sigma^2)$：

$$F(x)=P(X\leq x)=P\left(\dfrac{X-\mu}{\sigma}\leq\dfrac{x-\mu}{\sigma}\right)=\varPhi\left(\frac{x-\mu}{\sigma}\right)$$

$$P(x_1<X<x_2)=\varPhi\left(\frac{x_2-\mu}{\sigma}\right)-\varPhi\left(\frac{x_1-\mu}{\sigma}\right)$$

而所谓**上$\alpha$分位数**，是指下式中：

$$P(X>z_\alpha)=\alpha\quad,\quad 0<\alpha<1$$

若$X$服从**标准正态分布**，则$z_\alpha$称为标准正态分布的**上$\alpha$分位数**。

---

## 四、随机变量的期望与方差

### 1.期望

简便起见，我们从离散型随机变量入手。我们知道平均数的概念，即一组具有$n$个值的集合$\{a_1,a_2,...,a_n\}$，其平均值为：

$$\bar a=\dfrac{a_1+a_2+...+a_n}{n}$$

若每个值具有权重$\{w_1,w_2,...,w_n\}$，那么其**加权平均数**为：

$$\bar a=\dfrac{a_1w_1+a_2w_2+...+a_nw_n}{n}=a_1\cdot\dfrac{w_1}{n}+a_2\cdot\dfrac{w_2}{n}+...+a_n\cdot\dfrac{w_n}{n}$$

概率论中的**数学期望**，简称**期望**，或称**均值**，就是以概率为权的变量取值的加权平均数。即若离散型随机变量$X$具有取值集合$\{x_1,x_2,...,x_n,...\}$，各取值的概率是$\{p_1,p_2,...,p_n,...\}$，那么$X$的期望$E(X)$是：

$$E(X)=x_1p_1+x_2p_2+...+x_np_n+...=\displaystyle\sum_{i=1}^{+\infty}x_ip_i$$

对于连续型随机变量，虽然某取值点的概率为$0$，但是在小区间上的概率不为$0$。任意小区间上的概率是$dp=f(x)dx$，以此为权，那么这一小区间上的取值加权后就是$dE(X)=x\cdot dp=xf(x)dx$。两端积分即得连续型随机变量的期望计算公式：

$$E(X)=\displaystyle\int_{-\infty}^{+\infty}xf(x)\ dx$$

注意到上面的求和和积分是**无穷级数**和**反常积分**。当求和或积分**绝对收敛**的时候，称数学期望存在。

对于数学期望，存在四个基本公式：

(1)对于常数$C$，$E(C)=C$；

(2)$E(CX)=CE(X)$；

(3)设有两个随机变量$X,Y$，那么$E(X+Y)=E(X)+E(Y)$；

(4)若两个随机变量$X,Y$**相互独立**，那么$E(XY)=E(X)E(Y)$。

### 2.方差

**方差**用于刻画一组取值偏离均值的程度。在经典的数学中，取值集合$\{a_1,a_2,...,a_n\}$的方差如下计算：

$$\dfrac{(a_1-\bar{a})^2+(a_2-\bar{a})^2+...+(a_n-\bar{a})^2}{n}$$

如果取值具有权重$\{w_1,w_2,...,w_n\}$，那么方差计算为：

$$\dfrac{(a_1-\bar{a})^2w_1+(a_2-\bar{a})^2w_2+...+(a_n-\bar{a})^2w_n}{n}$$

显然这也是一种加权平均数。对应到概率论中的概念，方差亦是一种期望，即：

$$D(X)=E((X-E(X))^2)$$

而**标准差**是方差的平方根，即$\sigma(X)=\sqrt{D(X)}$，这个字母在之前的正态分布中出现过。

上述的公式针对离散型随机变量。若是连续型随机变量，举一反三地写出：

$$D(X)=\displaystyle\int_{-\infty}^{+\infty}(x-E(X))^2f(x)\ dx$$

有时根据定义式计算起来会比较麻烦，因此方差可以用一种等价形式计算：

$$D(X)=E(X^2)-E^2(X)$$

这可以直接由期望的基本公式推得。

对于任意的随机变量$X$，若$E(X)=\mu,D(X)=\sigma^2$，那么总可以通过线性变换使新变量$Y$存在$E(X)=0,D(X)=1$。这种线性变换已经在上述的正态分布中出现过，即$Y=\dfrac{X-\mu}{\sigma}$，这个新变量称为$X$的**标准化变量**。

由期望的基本公式，也可以推得方差的基本公式：

(1)对于常数$C$，$D(C)=0$；

(2)$D(CX)=C^2E(X)$；

(3)设有两个随机变量$X,Y$，那么$D(X+Y)=D(X)+D(Y)+2E[(X-E(X))(Y-E(Y))]$；

(4)若两个随机变量$X,Y$**相互独立**，那么$D(X+Y)=D(X)+D(Y)$，即$E[(X-E(X))(Y-E(Y))]=0$。

>关于两个随机变量$X,Y$的期望$E[(X-E(X))(Y-E(Y))]$称为**协方差**，简记为$Cov(X,Y)$，它可以展开为$Cov(X,Y)=E(XY)-E(X)E(Y)$。关于协方差等相关的内容会在之后的**多维随机变量**中讲述。

### 3.切比雪夫不等式

**切比雪夫不等式**具有如下的形式。设有随机变量$X$，且$E(X)=\mu,D(X)=\sigma^2$，那么对于任意正的取值$\varepsilon$：

$$P(|X-\mu|\geq\varepsilon)\leq\dfrac{\sigma^2}{\varepsilon^2}$$

这个不等式常用于未知概率密度或分布律，但已知期望方差时的概率估计。

### 4.几种分布的期望与方差

现在我们来统一分别计算上述六种分布的期望与方差。

**(1)伯努利分布**：

显然$E(X)=0\cdot (1-p)+1\cdot p=p$

$E(X^2)=0^2\cdot (1-p)+1^2\cdot p=p$

$D(X)=E(X^2)-E^2(X)=p-p^2=p(1-p)$

**(2)二项分布**：

由于二项分布是**独立重复**次的伯努利试验的结果，每一次试验的结果都是伯努利分布，显然可以记这$n$次试验的结果为$\{X_1,X_2,...,X_n\}$，那么二项分布的结果$X=X_1+X_2+...+X_n$。

因此，$E(X)=E(\sum X_i)=E(X_1)+E(X_2)+...+E(X_n)=np$

$D(X)=\sum D(X_i)=np(1-p)$

**(3)泊松分布**：

在理想情况下，二项分布要进化为泊松分布需要满足条件$\lambda=np,n\rightarrow+\infty,p\rightarrow0$。将此代入二项分布的期望与方差就能得到$E(X)=D(X)=\lambda$。这表明泊松分布的期望与方差是相等的。

当然，如果不用这样取巧的方法而直接计算，也能得到同样的结果：

$$\begin{aligned}
    E(X)&=\displaystyle\sum_{k=1}^{+\infty}kP(X=k)\\
    &=\sum_{k=1}^{+\infty}k\cdot\dfrac{\lambda^ke^{-\lambda}}{k!}\\
    &=\lambda e^{-\lambda}\sum_{k=1}^{+\infty}\dfrac{\lambda^{k-1}}{(k-1)!}\\
    &=\lambda e^{-\lambda}\cdot e^{\lambda}\\
    &=\lambda
\end{aligned}$$

$$\begin{aligned}
    E(X^2)&=\displaystyle\sum_{k=1}^{+\infty}k^2P(X=k)\\
    &=\sum_{k=1}^{+\infty}k^2\cdot\dfrac{\lambda^ke^{-\lambda}}{k!}\\
    &=\lambda e^{-\lambda}\sum_{k=1}^{+\infty}\dfrac{k\lambda^{k-1}}{(k-1)!}\\
    &=\lambda e^{-\lambda}\sum_{k=1}^{+\infty}\dfrac{(k-1)\lambda^{k-1}+\lambda^{k-1}}{(k-1)!}\\
    &=\lambda e^{-\lambda}\left( \lambda\sum_{k=1}^{+\infty}\dfrac{\lambda^{k-2}}{(k-2)!}+\sum_{k=1}^{+\infty}\dfrac{+\lambda^{k-1}}{(k-1)!} \right)\\
    &=\lambda e^{-\lambda}\cdot (\lambda e^{\lambda}+e^{\lambda})\\
    &=\lambda^2+\lambda
\end{aligned}$$

$$D(X)=E(X^2)-E^2(X)=\lambda^2+\lambda-\lambda^2=\lambda$$

**(4)均匀分布**：

回忆均匀分布的概率密度是$f(x)=\dfrac{1}{b-a},\ a<x<b$。则：

$$\begin{aligned}
    E(X)&=\displaystyle\int_{-\infty}^{+\infty}xf(x)\ dx\\
    &=\int_{a}^{b}x\cdot\dfrac{1}{b-a}\ dx\\
    &=\dfrac{b^2-a^2}{2}\cdot\dfrac{1}{b-a}\\
    &=\dfrac{a+b}{2}
\end{aligned}$$

$$\begin{aligned}
    E(X^2)&=\displaystyle\int_{-\infty}^{+\infty}x^2f(x)\ dx\\
    &=\int_{a}^{b}x^2\cdot\dfrac{1}{b-a}\ dx\\
    &=\dfrac{b^3-a^3}{3}\cdot\dfrac{1}{b-a}\\
    &=\dfrac{a^2+ab+b^2}{3}
\end{aligned}$$

$$D(X)=E(X^2)-E^2(X)=\dfrac{(a-b)^2}{12}$$

**(5)指数分布**：

回忆指数分布的概率密度是$f(x)=\dfrac{1}{\theta}e^{-\frac{x}{\theta}},\ x>0$。则：

$$\begin{aligned}
    E(X)&=\displaystyle\int_{-\infty}^{+\infty}xf(x)\ dx\\
    &=\int_{0}^{+\infty}x\cdot\dfrac{1}{\theta}e^{-\frac{x}{\theta}}\ dx\\
    &=\theta[(u-1)e^u]_{0}^{-\infty}\\
    &=\theta
\end{aligned}$$

$$\begin{aligned}
    E(X^2)&=\displaystyle\int_{-\infty}^{+\infty}x^2f(x)\ dx\\
    &=\int_{0}^{+\infty}x^2\cdot\dfrac{1}{\theta}e^{-\frac{x}{\theta}}\ dx\\
    &=-\theta^2[(u^2-2u+2)e^u]_{0}^{-\infty}\\
    &=2\theta^2
\end{aligned}$$

$$D(X)=E(X^2)-E^2(X)=\theta^2$$

**(6)正态分布**：

正态分布的期望和方差就是它的参数$\mu,\sigma^2$。但是，由于两条基本公式$E(X+Y)=E(X)+E(Y),D(X+Y)=D(X)+D(Y)$(其中$X,Y$相互独立)的存在，两个相互独立但服从正态分布的随机变量可以直接线性叠加为一个新的正态分布变量。例如假设$X\sim N(\mu_1,\sigma_1^2),Y\sim N(\mu_2,\sigma_2^2)$，那么若$Z=C_1X+C_2Y$：

$$E(Z)=C_1E(X)+C_2E(Y)=C_1\mu_1+C_2\mu_2$$

$$D(Z)=C_1^2D(X)+C_2^2D(Y)=C_1^2\sigma_1^2+C_2^2\sigma_2^2$$

$$Z\sim N(C_1\mu_1+C_2\mu_2,C_1^2\sigma_1^2+C_2^2\sigma_2^2)$$

---

## 五、随机变量的函数的分布与期望

这一部分是概率论中的难点之一，但只要牢记**“分布函数是概率与概率密度之间的桥梁”**这句话即可。

假设有一个随机变量$Y$是关于$X$的函数：$Y=g(X)$。那么，如何根据$X$的概率密度来求$Y$的分布与期望？(离散型随机变量情况下较为简单，这里就不赘述)

### 1.分布

为了区分，我们将$X,Y$的分布函数和概率密度分别记为$F_X(x),F_Y(y)$，$f_X(x),f_Y(y)$。那么，针对$F_Y(y)$运算：

$$F_Y(y)=P(Y\leq y)=P(g(X)\leq y)$$

由$g(X)\leq y$可以反解出$X$的**取值范围**。有时这个方程会同时给出$X$的上界和下界，但在$g(\cdot)$严格单调的情况下$X$的取值范围应只有上界或只有下界。例如，如果只有上界：

$$F_Y(y)=P(g(X)\leq y)=P(X\leq g^{-1}(y))=F_X(g^{-1}(y))$$

其中$g^{-1}(y)$是$g(x)$的**反函数**，不妨记为$h(y)$。因此，$Y$的分布函数只需要将$F_X(x)$中的$x$都替换为反函数$h(y)$即可(在$g(\cdot)$单调并且仅确定$X$的上界的情况下)。

由此，直接对$F_Y(y)$求导，即可得$Y$的概率密度：

$$f_Y(y)=F_Y'(y)=F_X'(h(y))\cdot|h'(y)|=f_X(h(y))\cdot|h'(y)|$$

上式只要求$g(x)$是**严格单调函数**，绝对值的引入使其囊括了递增和递减的情况。但如果$g(x)$不是严格单调函数，还需要像上面那样一步步通过分布函数递推。

### 2.期望

期望只需要简单地代入原期望计算公式即可，可以免去计算新变量的概率密度或分布函数：

$$E(X)=\displaystyle\int_{-\infty}^{+\infty}xf(x)\ dx$$

$$E(Y)=E(g(X))=\displaystyle\int_{-\infty}^{+\infty}g(x)f(x)\ dx$$
