# 高等代数复习笔记

## 一、正交投影(Orthogonal Projection)

### 1.正交分解定理(Decomposition)

若$W$是$R^n$的子空间，那么$R^n$中每一个向量$y$都能表示为$\bm y=proj_W\bm y+\bm z$。

其中，$proj_W\bm y$属于$W$，$z$属于$W^{\bot}$。

如果$\{u_1,u_2...u_k\}$是$W$的任意正交基，那么：

$$proj_W\bm y=\dfrac{y\cdot u_1}{u_1\cdot u_1}u_1+...+\dfrac{y\cdot u_k}{u_k\cdot u_k}u_k$$

### 2.正交投影性质

若$y\in W=Span\{u_1,u_2...u_k\}$，那么$proj_W\bm y=y$。由此得到最佳逼近定理：

### 3.最佳逼近定理(The Best Approximation)

若$W$是$R^n$的子空间，那么对于$R^n$中任一向量$y$，$proj_W\bm y$是$W$中最接近$y$的点。

### 4.单位正交基情况下的简化

若有单位正交基$\{u_1,u_2...u_k\}$，则：

$$proj_W\bm y=(y\cdot u_1)u_1+...+(y\cdot u_k)u_k$$

若记$U=[u_1,u_2...u_k]$，则：

$$proj_W\bm y=UU^Ty$$

---

## 二、施密特正交化(Gram-Schmidt Process)

### 1.正交化方法

对$R^n$中一个子空间的基$\{x_1,x_2...x_k\}$，按以下序列给出正交基：

$$\begin{cases}
    &v_1=x_1\\
    &v_2=x_2-\dfrac{x_2\cdot v_1}{v_1\cdot v_1}v_1\\
    &v_3=x_3-\dfrac{x_3\cdot v_1}{v_1\cdot v_1}v_1-\dfrac{x_3\cdot v_2}{v_2\cdot v_2}v_2\\
    &...\\
    &v_k=x_k-\dfrac{x_k\cdot v_1}{v_1\cdot v_1}v_1-...-\dfrac{x_k\cdot v_{k-1}}{v_{k-1}\cdot v_{k-1}}v_{k-1}\\
\end{cases}$$

上述向量单位化后，即可形成单位正交基/标准正交基(orthonormal basis)。

### 2.QR分解(QR Factorization)

若矩阵$A_{m\times n}$之列向量线性无关，那么$A=QR$，其中$Q_{m\times n}$的列是$Col A$的标准正交基，$R_{n\times n}$是一个上三角可逆的对角线元素为正的矩阵。

显然由于$Q$是标准正交矩阵，那么$Q^T=Q$,$R=Q^TA$

---

## 三、最小二乘问题(Least Squares)

### 1.定义

若$A_{m\times n},\ \bm b\in R^m$，那么$A\bm x=\bm b$的最小二乘解是$R^n$中的$\hat x$，使得：

$$||b-A\hat x||\leq||b-Ax||$$

总成立。

### 2.一般最小二乘问题的解

由最佳逼近定理，上述问题的解集与：

$$A^TA\bm x=A^T\bm b$$

的解集一致。

例如：

<image src="image/高等代数/6.5.1.png">

### 3.最小二乘误差(error)

$\bm b$到$A\hat x$的距离，即$||\bm b -A\hat x||$为误差。

### 4.QR分解

若$A_{m\times n}=QR$具有线性无关的列，那么$A\bm x=\bm b$的唯一最小二乘解是：

$$\hat {\bm x}=R^{-1}Q^T\bm b$$

---

## 四、对称矩阵的对角化(Diagonalization of Symmetric Matrices)

对称矩阵是$A^T=A$的方阵。

### 1.对角化过程

<image src="image/高等代数/7.1.1.png">

$A=PDP^{-1}=PDP^T$

### 2.谱分解(Spectral Decompositon)

假设$A=PDP^T$，那么：

$$A=PDP^T=[u_1,...,u_n]\begin{bmatrix}
    \lambda_1& &0\\
    &\ddots &\\
    0& &\lambda_n
\end{bmatrix}\begin{bmatrix}
    u_1^T\\
    \vdots\\
    u_n^T\\
\end{bmatrix}=[\lambda_1u_1,...,\lambda_nu_n]\begin{bmatrix}
    u_1^T\\
    \vdots\\
    u_n^T\\
\end{bmatrix}\\
=\lambda_1u_1u_1^T+...+\lambda_nu_nu_n^T$$

---

## 五、二次型(Quadratic Form)

### 1.变量代换

若$\bm x$是$R^n$中的变量向量，那么$\bm x=P\bm y$，$\bm y$是$R^n$中的新向量。因此，二次型可表示为：

$\bm x^TA\bm x=\bm y^T(P^TAP)\bm y$

$P^TAP$是新的二次型矩阵。

### 2.二次型的分类

如果一个二次型的所有特征值都是正的，那么它是正定的(positive definite)。

都是负数，它是负定的(negtive definite)。

否则，是不定的(indefinite)。

---

## 六、条件优化(Constrained Optimization)

定义如下代数：

$$m=min\{x^TAx,\ ||x||=1\},\quad M=max\{x^TAx,\ ||x||=1\}$$

### 1.

设$A$是对称矩阵，那么$m,M$分别是最小和最大的特征值，并且在分别对应的特征向量上二次型$x^TAx$取得最小最大值$m,M$。

### 2.

若在$x^Tx=1,x^Tu_1=0$的限制下，$x^TAx$的最大值是第二大的特征值，并在相应的特征向量下取得此值。这个结论可递推。

---

## 七、奇异值分解(Singular Value Decomposition)

### 1.奇异值

$A$的奇异值是$A^TA$的特征值的平方根。一般记为$\sigma_1,...\sigma_n$并且按递减顺序排列。

如果$\{\lambda_1,...,\lambda_n\}$是递减的特征值，
$\{\bm v_1,...,\bm v_n\}$是相应的标准单位正交基。若$A$有$r$个非零奇异值，那么$\{A\bm v_1,...,A\bm v_r\}$是$Col A$的一个正交基，并且$rank(A)=r$。

### 2.奇异值分解

设有$rank(A_{m\times n})=r$，那么可将其分解为$A=U\Sigma V^T$。

其中，$\Sigma_{m\times n}=\begin{bmatrix}
    D&0\\
    0&0\\
\end{bmatrix}$，$D$的对角线元素是$A$的前$r$个奇异值；$U_{m\times m}$是正交矩阵，$V_{n\times n}$是正交矩阵。

$U$的列称为$A$的左奇异向量，$V$的列称为$A$的右奇异向量。

$A_{m\times n}$奇异值的分解分三步：

(1)计算$A^TA$，将其正交对角化。

(2)计算$V,\Sigma$。将特征值降序排列，对应的单位正交特征向量集就是右奇异向量，构成$V$。之后取前$r$个非零奇异值够成$D$，再构成与$A$同行列的$Sigma$。

(3)计算$U$。$U$的前$r$列是$\{A\bm v_1,...,A\bm v_r\}$构成的单位向量，即$\bm u_k=\dfrac{1}{\sigma_k}A\bm v_k$
